library("zoo")
library("TTR")


SPY <- read.csv("~/Desktop/PROJECT_1/IQ_data/SPY.csv", header=FALSE)
SPY$price<-(SPY$V2+SPY$V3+SPY$V5)/3
SPY$R1<-SPY$price*2-SPY$V3
SPY$R2<-SPY$price+(SPY$V2-SPY$V3)
SPY$S1<-SPY$price*2-SPY$V2
SPY$S2<-SPY$price-(SPY$V2-SPY$V3)
VP<-rollapply(SPY$price*SPY$V7, width =12,FUN = mean,fill=NA, align = "right")
V<-rollapply(SPY$V7, width =12,FUN = mean,fill=NA, align = "right")
SPY$VWAP<-VP/V

## Compute the price change for the stock and classify as UP/DOWN
price <- SPY$V4-lag(SPY$VWAP)
SPY$UD <- ifelse(price > 0,"UP","DOWN")

## break out and break down
priceU <- SPY$V4-SPY$R2
priceD <- SPY$S2-SPY$V4
SPY$BOD <- ifelse(priceU > 0,"BO",ifelse(priceD>0,"BD","N"))

SPY$price<-rollapply(SPY$price, width =60,FUN = mean,fill=NA, align = "right")

###///

df_stock <- read.csv("~/Desktop/PROJECT_1/IQ_data/SPY.csv", header=FALSE)


#Strategy: if Open price above R1 and crosses VWAP then buy and open above R2 then sell.if VWAP above R2+0.04 or above mean+2*SD+0.05 line the sell short 
SPY<-df_stock
price<-(SPY$V2+SPY$V3+SPY$V5)/3
R1<-price*2-SPY$V3
R2<-price+(SPY$V2-SPY$V3)
S1<-price*2-SPY$V2
S2<-price-(SPY$V2-SPY$V3)
VP<-rollapply(price*SPY$V7, width =12,FUN = mean,fill=NA, align = "right")
V<-rollapply(SPY$V7, width =12,FUN = mean,fill=NA, align = "right")
VWAP<-VP/V
ma<-rollapply(price, width =30,FUN = mean,fill=NA, align = "right")
sd<-rollapply(price, width =30,FUN = sd,fill=NA, align = "right")

UL<-ma+2*sd
DL<-ma-2*sd
UD<-ifelse(SPY$V3>R1,"UP",ifelse(SPY$V3<S1,"DN","N"))
H<-ifelse(R2>UL,R2,UL)
L<-ifelse(S2<DL,S2,DL)
BO<-ifelse(SPY$V3>H,"BO",ifelse(SPY$V3<L,"BD","N"))




library(quantmod); library(TTR); library(caret);library(corrplot);library(pROC);library(FSelector);

## Use set.seed function to ensure the results are repeatable
set.seed(5)

## Read the stock and index data
df_stock = read.csv("BAJAJ-AUTO 5 Yr data.csv")
df_index = read.csv("NIFTY 5 Yr data.csv")
//df_stock<- read.csv("~/Desktop/pred_mod_R/BA.csv")
//df_index<- read.csv("~/Desktop/pred_mod_R/NIFTY.csv")

## Compute the price change for the stock and classify as UP/DOWN
price = df_stock$Last-df_stock$Open
class = ifelse(price > 0,"UP","DOWN")

## Compute the various technical indicators that will be used 
# Force Index Indicator
forceindex = (df_stock$Last - df_stock$Open) * df_stock$Vol ; 
forceindex = c(NA,head(forceindex,-1)) ;

# Buy & Sell signal Indicators (Williams R% and RSI)
WillR5  = WPR(df_stock[,c("High","Low","Last")], n = 5) ; WillR5 = c(NA,head(WillR5,-1)) ;
WillR10 = WPR(df_stock[,c("High","Low","Last")], n = 10) ; WillR10 = c(NA,head(WillR10,-1)) ;
WillR15 = WPR(df_stock[,c("High","Low","Last")], n = 15) ; WillR15 = c(NA,head(WillR15,-1)) ;

RSI5  = RSI(df_stock$Last, n = 5,maType="WMA") ;RSI5 = c(NA,head(RSI5,-1)) ;
RSI10 = RSI(df_stock$Last, n = 10,maType="WMA") ;RSI10 = c(NA,head(RSI10,-1)) ;
RSI15 = RSI(df_stock$Last, n = 15,maType="WMA") ;RSI15 = c(NA,head(RSI15,-1)) ;

# Price change Indicators (ROC and Momentum)
ROC5 = ROC(df_stock$Last, n = 5,type ="discrete")*100 ; ROC5 = c(NA,head(ROC5,-1)) ;
ROC10 = ROC(df_stock$Last, n = 10,type ="discrete")*100 ; ROC10 = c(NA,head(ROC10,-1)) ;

MOM5 = momentum(df_stock$Last, n = 5, na.pad = TRUE) ; MOM5 = c(NA,head(MOM5,-1)) ;
MOM10 = momentum(df_stock$Last, n = 10, na.pad = TRUE) ; MOM10 = c(NA,head(MOM10,-1)) ;

MOM5Indx = momentum(df_index$Last, n = 5, na.pad = TRUE) ; MOM5Indx = c(NA,head(MOM5Indx,-1)) ;
MOM10Indx = momentum(df_index$Last, n = 10, na.pad = TRUE); MOM10Indx = c(NA,head(MOM10Indx,-1)) ;

# Volatility signal Indicator (ATR)
ATR5 = ATR(df_stock[,c("High","Low","Last")], n = 5, maType="WMA")[,1] ; ATR5 = c(NA,head(ATR5,-1)) ;
ATR10 = ATR(df_stock[,c("High","Low","Last")], n = 10, maType="WMA")[,1]; ATR10 = c(NA,head(ATR10,-1)) ;

ATR5Indx = ATR(df_index[,c("High","Low","Last")], n = 5, maType="WMA")[,1]; ATR5Indx = c(NA,head(ATR5Indx,-1)) ;
ATR10Indx = ATR(df_index[,c("High","Low","Last")], n = 10, maType="WMA")[,1]; ATR10Indx = c(NA,head(ATR10Indx,-1));

## Combining all the Indicators and the Class into one dataframe
dataset = data.frame(class,forceindex,WillR5,WillR10,WillR15,RSI5,RSI10,RSI15,ROC5,ROC10,MOM5,MOM10,ATR5,ATR10,MOM5Indx,MOM10Indx,ATR5Indx,ATR10Indx)
//dataset<-data.frame(class,forceindex,WillR5,WillR10,WillR15,RSI5,RSI10,RSI15,ROC5,ROC10,MOM5,MOM10,ATR5,ATR10,MOM5Indx,MOM10Indx,ATR5Indx,ATR10Indx)
dataset = na.omit(dataset)

## Understanding the dataset using descriptive statistics
print(head(dataset),5)
dim(dataset)
y = dataset$class
cbind(freq=table(y), percentage=prop.table(table(y))*100)
//cbind(freq=table(y),percentage=prop.table(table(y))*100)
summary(dataset)

##  Visualizing the dataset using a correlation matrix
correlations = cor(dataset[,c(2:18)])
print(head(correlations))
corrplot(correlations, method="circle")

## Selecting features using the random.forest.importance function from the FSelector package
set.seed(5)
weights = random.forest.importance(class~., dataset, importance.type = 1)
print(weights)

set.seed(5)
subset = cutoff.k(weights, 10)
print(subset)

## Creating a dataframe using the selected features
dataset_rf = data.frame(class,forceindex,WillR5,WillR10,RSI5,RSI10,RSI15,ROC5,ROC10,MOM5,MOM10Indx)
dataset_rf = na.omit(dataset_rf)

# Resampling method used - 10-fold cross validation 
# with "Accuracy" as the model evaluation metric.
trainControl <- trainControl(method="cv", number=10)
metric = "Accuracy"

## Trying four different Classification algorithms
# k-Nearest Neighbors (KNN)
set.seed(5)


fit.knn <- train(class~., data=dataset_rf, method="knn",metric=metric, preProc=c("range"),trControl=trainControl)

# Classification and Regression Trees (CART)
set.seed(5)
fit.cart <- train(class~., data=dataset_rf, method="rpart",metric=metric,preProc=c("range"),trControl=trainControl)

# Naive Bayes (NB)
set.seed(5)
fit.nb<-train(class~., data=dataset_rf, method="nb",metric=metric, preProc=c("range"),trControl=trainControl)

# Support Vector Machine with Radial Basis Function (SVM)
set.seed(5)
fit.svm <- train(class~., data=dataset_rf, method="svmRadial",metric=metric,preProc=c("range"),trControl=trainControl)

## Evaluating the algorithms using the "Accuracy" metric
results <- resamples(list(KNN=fit.knn,CART=fit.cart, NB=fit.nb, SVM=fit.svm))
summary(results)
dotplot(results)

## Tuning the shortlisted algorithm (KNN algorithm)
set.seed(5)
grid <- expand.grid(.k=seq(1,10,by=1))
fit.knn <- train(class~., data=dataset_rf, method="knn", metric=metric, tuneGrid=grid,preProc=c("range"), trControl=trainControl)
print(fit.knn)



////#####
Step 1: Feature construction

Our process commences with the construction of a dataset that contains the features which will be used to make the predictions, and the output variable.

First, we build our dataset using raw data comprising of a 5-year price series for a stock and an index. This stock and index data consists of Date, Open, High, Low, Last and Volume. Using this data we compute our features based on various technical indicators (listed below).

No	Name of the Technical Indicator
1	Force Index(FI)
2	William %R
3	Relative Strength Index(RSI)
4	Rate Of Change(ROC)
5	Momentum (MOM)
6	Average True Range(ATR)
Note: Before you begin, make sure that you have the following packages installed and selected on your RStudio: Quantmode, PRoc, TTR, Caret, Corrplot, FSelector, rJava, kLar, randomforest, kernlab, rpart

The computed technical indicators along with the price change class (Up/Down) are combined to form a single dataset.

Step 2: Understanding the dataset using numbers and visuals

The most significant pre-requisite for predictive modeling is to have a good understanding of the dataset. The understanding helps in:

Data transforms
Choosing the right machine learning algorithms
Explains the results obtained from the model
Improves its accuracy
To attain this understanding of the dataset, you can use descriptive statistics like standard deviation, mean, skewness, along with graphical understanding of the data.

R Code:

Step 3: Feature selection

Feature selection is the process of selecting a subset of features that are most relevant for model construction which aid in creating an accurate predictive model. There are a wide range of feature selection algorithms, and these mainly fall in one of the three categories:

Filter method– selects features by assigning a score to them using some statistical measure.
Wrapper method– evaluates different subset of features, and determines the best subset.
Embedded method – This method figures out which of the features give the best accuracy while the model is being trained.
 

In our model, we will use filter method utilising the random.forest.importance function from the FSelector package. The random.forest.importance function rates the importance of each feature in the classification of the outcome, i.e. class variable. The function returns a data frame containing the name of each attribute and the importance value based on the mean decrease in accuracy.

Now, in order to choose the best features using the importance values returned by random.forest.importance, we use the cutoff.k function which provides k features with the highest importance values.

In our model, we have selected ten out of the seventeen features that were initially extracted from the price data series. Using these ten features we create a dataset that will be used on the machine learning algorithms.

Step 4: Set the Resampling method

In Predictive modeling we need data for two reasons:

To train the model
To test data to determine the accuracy of the predictions made by the model.
When we have a limited data we can use resampling methods which split data into training and testing parts. There are different resampling methods available in R such as data splitting, bootstrap method, k-fold cross validation, Repeated k-fold cross validation etc.

In our example, we are using k-fold cross validation method which splits the dataset into k-subsets. We keep each subset aside while the model trains on all the remaining subsets. This process repeats until it determines accuracy for each instance in the dataset. Finally, the function determines an overall accuracy estimate.

Step 5: Training different algorithms

There are hundreds of machine learning algorithms available in R, and determining which model to use can be confusing for beginners. Modelers are expected to try different algorithms based on the problem at hand and with more experience & practice you will be able to determine the right set.

Few kinds of problems:

Classification
Regression
Clustering
Rule Extraction
In our example we are dealing with a classification problem, hence we will explore some algorithms that are geared to solve such problems.  We will be using the following:

k-Nearest Neighbors (KNN)
Classification and Regression Trees (CART)
Naive Bayes (NB)
Support Vector Machine with Radial Basis Function (SVM) algorithms.
We won’t be detailing the working of these algorithms as the objective of the post is to detail the steps in the modeling process, and not the underlying working of these algorithms.

We use the train function from the caret package which fits different predictive models using a grid of tuning parameters. For example:

In the above example, we are using the KNN algorithm which is specified via the method argument. class is the output variable, dataset_rf is the dataset that is used to train and test the model. The preProc argument defines the data transform method, while the trControl argument defines the computational nuances of the train function.

Step 6: Evaluating the Models

The trained models are evaluated for their accuracy in predicting the outcome using different metrics like Accuracy, Kappa, Root Mean Squared Error (RMSE), R2etc.

We are using the “Accuracy” metric to evaluate our trained models. Accuracy is the percentage of correctly classified instances out of all instances in the test dataset. We have used the resamples function from the caret package which takes the trained objects, and produces a summary using the summary function.

Step 7: Tuning the shortlisted model

As we can observe from the accuracy metric, all the models have accuracy between 50-54%. Ideally we should try to tune models with highest accuracies. However, for the example’s sake, we will select the KNN algorithm and try to improve its accuracy by tuning the parameters.

Tuning the parameters

We will tune the KNN algorithm (parameter k) over values ranging from 1 to 10. The tuneGrid argument in the train function helps determine the accuracy over this range when evaluate the model.

As can be seen from the tuning process, the accuracy of KNN algorithm has not increased and it is similar to the accuracy obtained earlier. You can try tuning other algorithms as well based on their respective tuning parameters, and select the algorithm with the best accuracy.

Conclusion:
In the initial stages, modelers can try working with different technical indicators, create models based on other asset classes, and can try out different predictive modeling problems.

With experience and practice, you will start building robust models to trade profitably in the markets.





