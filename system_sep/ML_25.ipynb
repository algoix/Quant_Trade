{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting & plotting stock prices.\n"
     ]
    }
   ],
   "source": [
    "import zmq\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy\n",
    "from numpy import inf\n",
    "\n",
    "import json\n",
    "import plotly_stream as plyst\n",
    "import plotly.tools as plyt\n",
    "import plotly.plotly as ply\n",
    "#!pip install plotly\n",
    "import tpqib\n",
    "import datetime\n",
    "\n",
    "#from sklearn.cross_validation import train_test_split\n",
    "from sklearn import linear_model\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pickle\n",
    "\n",
    "iterations = 0\n",
    "\n",
    "context = zmq.Context()\n",
    "\n",
    "#Forwarding ML output\n",
    "socket_pub = context.socket(zmq.PUB)\n",
    "socket_pub.bind('tcp://127.0.0.1:7010')\n",
    "\n",
    "port = \"7000\"\n",
    "# socket to talk to server\n",
    "socket_sub = context.socket(zmq.SUB)\n",
    "print (\"Collecting & plotting stock prices.\")\n",
    "socket_sub.connect(\"tcp://localhost:%s\" % port)\n",
    "\n",
    "socket_sub.setsockopt_string(zmq.SUBSCRIBE, u'SPY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "pdf= pd.DataFrame()\n",
    "final=pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_ml=pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def preprocessing(df):\n",
    "    df.bidPrice=df.loc[:,'bidPrice'].replace(to_replace=0, method='ffill')\n",
    "    df.bidSize=df.loc[:,'bidSize'].replace(to_replace=0, method='ffill')\n",
    "    df.askPrice=df.loc[:,'askPrice'].replace(to_replace=0, method='ffill')\n",
    "    df.askSize=df.loc[:,'askSize'].replace(to_replace=0, method='ffill')\n",
    "    #df=df.dropna()\n",
    "    # to exclude 0\n",
    "    #df=df[df['bidPrice']>df.bidPrice.mean()-df.bidPrice.std()]\n",
    "    #df=df[df['askPrice']>df.askPrice.mean()-df.askPrice.std()]\n",
    "    df['mid']=(df.askPrice+df.bidPrice)/2\n",
    "    df['vwap']=((df.loc[:,'bidPrice']*df.loc[:,'bidSize'])+(df.loc[:,'askPrice']*df.loc[:,'askSize']))/(df.loc[:,'bidSize']+df.loc[:,'askSize'])\n",
    "    df['spread']=df.vwap-(df.askPrice+df.bidPrice)/2\n",
    "    df['v']=(df.askPrice+df.bidPrice)/2-((df.askPrice+df.bidPrice)/2).shift(60)\n",
    "    df['return']=(df.askPrice/df.bidPrice.shift(1))-1\n",
    "    df['sigma']=df.spread.rolling(60).std()\n",
    "    return df\n",
    "\n",
    "def normalise(df,window_length=60):\n",
    "    dfn=(df-df.rolling(window_length).min())/(df.rolling(window_length).max()-df.rolling(window_length).min())\n",
    "    return dfn\n",
    "\n",
    "def normalise_z(df,window_length=12):\n",
    "    dfn=(df-df.rolling(window_length).mean())/(df.rolling(window_length).std())\n",
    "    return dfn\n",
    "\n",
    "\n",
    "def de_normalise(data,df,window_length=60):\n",
    "    dn=(df*(data.rolling(window_length).max()-data.rolling(window_length).min()))+data.rolling(window_length).min()\n",
    "    return dn\n",
    "\n",
    "#https://stackoverflow.com/questions/312443/how-do-you-split-a-list-into-evenly-sized-chunks\n",
    "def chunks(l, n):\n",
    "    \"\"\"Yield successive n-sized chunks from l.\"\"\"\n",
    "    for i in range(0, len(l), n):\n",
    "        yield l[i:i + n]\n",
    "        \n",
    "##### ARIMA        \n",
    "\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "from statsmodels.tsa.arima_model import ARIMAResults\n",
    "        \n",
    "###ARIMA preprocessing\n",
    "def arima_processing(df):\n",
    "    #data=df[['vwap','mid']]\n",
    "    #df=df.dropna()\n",
    "    df['Lvwap']=np.log(df.vwap)\n",
    "    df['Lmid']=np.log(df.mid)\n",
    "    df['LDvwap']=df.Lvwap-df.Lvwap.shift(60)\n",
    "    df['LDmid']=df.Lmid-df.Lmid.shift(60)\n",
    "    #df=df.dropna()\n",
    "    return df   \n",
    "\n",
    "###Model is already saved from \"/Dropbox/DataScience/ARIMA_model_saving.ipynb\". Here loaded and added to \"df_ml\"\n",
    "def ARIMA_(data):\n",
    "    ### load model\n",
    "    #data=data.dropna()\n",
    "    #df=data[['Lvwap','Lmid']].tail(60)\n",
    "    df_arima=data\n",
    "    predictions_mid=ARIMA_mid(df_arima.LDmid)\n",
    "    predictions_vwap=ARIMA_vwap(df_arima.LDvwap) \n",
    "    df_arima['predictions_mid']=np.exp(float(predictions_mid[-1])+df_arima.LDmid.shift(60))\n",
    "    #df.predictions_mid=df.loc[:,'predictions_mid'].replace(to_replace='NaN', method='ffill')\n",
    "    df_arima['predictions_vwap']=np.exp(float(predictions_vwap[-1])+df_arima.LDvwap.shift(60))\n",
    "\n",
    "    df_ml['arima']=df_arima.predictions_mid+df_arima.mid-df_arima.predictions_vwap\n",
    "    return df_arima.predictions_mid+df_arima.mid-df_arima.predictions_vwap\n",
    "    \n",
    "def ARIMA_mid(data):\n",
    "    ### load model\n",
    "    \n",
    "    mid_arima_loaded = ARIMAResults.load('mid_arima.pkl')\n",
    "    predictions_mid = mid_arima_loaded.predict()\n",
    "    return predictions_mid\n",
    "\n",
    "def ARIMA_vwap(data):\n",
    "    ### load model\n",
    "    vwap_arima_loaded = ARIMAResults.load('vwap_arima.pkl')\n",
    "    predictions_vwap = vwap_arima_loaded.predict()\n",
    "    return predictions_vwap   \n",
    "\n",
    "def data_class(data):\n",
    "    #df_ml=df_ml.dropna()\n",
    "    data_cl=data.tail(len(df_ml))\n",
    "    a= np.where(df_ml.mid>df_ml.km,1,0)\n",
    "    b= np.where(df_ml.mid<df_ml.km,-1,0)\n",
    "    c=np.where(df_ml.mid>df_ml.arima,1,0)\n",
    "    d=np.where(df_ml.mid<df_ml.arima,-1,0)\n",
    "    e=np.where(df_ml.mid>df_ml.REG,1,0)\n",
    "    f=np.where(df_ml.mid<df_ml.REG,-1,0)\n",
    "    g=np.where(df_ml.mid>df_ml.SVR,1,0)\n",
    "    h=np.where(df_ml.mid<df_ml.SVR,-1,0)\n",
    "    data_cl['U']=np.where(a[-1]*c[-1]*e[-1]*g[-1]==1,1,0)\n",
    "    data_cl['D']=np.where(b[-1]*d[-1]*f[-1]*h[-1]==1,-1,0)\n",
    "    data_cl=data_cl.dropna()\n",
    "    return data_cl  \n",
    "\n",
    "#### KALMAN moving average\n",
    "\n",
    "##KF moving average\n",
    "#https://github.com/pykalman/pykalman\n",
    "\n",
    "# Import a Kalman filter and other useful libraries\n",
    "from pykalman import KalmanFilter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import poly1d\n",
    "\n",
    "def kalman_ma(data):\n",
    "    #x=data.mid\n",
    "    x=data.mid\n",
    "    # Construct a Kalman filter\n",
    "    kf = KalmanFilter(transition_matrices = [1],\n",
    "                  observation_matrices = [1],\n",
    "                  initial_state_mean = 248,\n",
    "                  initial_state_covariance = 1,\n",
    "                  observation_covariance=1,\n",
    "                  transition_covariance=.01)\n",
    "\n",
    "    # Use the observed values of the price to get a rolling mean\n",
    "    state_means, _ = kf.filter(x.values)\n",
    "    state_means = pd.Series(state_means.flatten(), index=x.index)\n",
    "    df_ml['km']=state_means\n",
    "    #return df_ml\n",
    "\n",
    "### Linear Regression, sklearn, svm:SVR,linear_model\n",
    "import pickle\n",
    "#from sklearn.cross_validation import train_test_split\n",
    "from sklearn import linear_model\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "## loading model saved from /Dropbox/DataScience/REG_model_saving.ipynb\n",
    "filename_rgr = 'rgr.sav'\n",
    "filename_svr = 'svr.sav'\n",
    "# load the model from disk\n",
    "loaded_rgr_model = pickle.load(open(filename_rgr, 'rb'))\n",
    "loaded_svr_model = pickle.load(open(filename_svr, 'rb'))\n",
    "\n",
    "def strat_lr(data,df):\n",
    "    df=df.dropna()\n",
    "    data=data.dropna()\n",
    "    X=df[['askPrice','askSize','bidPrice','bidSize','vwap','spread','v','return','sigma']]\n",
    "    y=df.mid\n",
    "    predict_regr=loaded_rgr_model.predict(X)\n",
    "    predict_svr=loaded_svr_model.predict(X)\n",
    "    df['predict_regr']=predict_regr\n",
    "    df['predict_svr']=predict_svr\n",
    "    df_ml['REG']=de_normalise(data.mid,df.predict_regr)\n",
    "    df_ml['SVR']=de_normalise(data.mid,df.predict_svr)\n",
    "    #return df_ml\n",
    "    \n",
    "#### loading classification model from /Dropbox/DataScience/ML_20Sep\n",
    "filename_svm_model_up = 'svm_model_up.sav'\n",
    "filename_lm_model_up = 'lm_model_up.sav'\n",
    "filename_svm_model_dn = 'svm_model_dn.sav'\n",
    "filename_lm_model_dn = 'lm_model_dn.sav'\n",
    "# load the model from disk\n",
    "loaded_svm_up_model = pickle.load(open(filename_svm_model_up, 'rb'))\n",
    "loaded_lm_up_model = pickle.load(open(filename_lm_model_up, 'rb'))\n",
    "loaded_svm_dn_model = pickle.load(open(filename_svm_model_dn, 'rb'))\n",
    "loaded_lm_dn_model = pickle.load(open(filename_lm_model_dn, 'rb'))\n",
    "\n",
    "def classification_up_dn(data):\n",
    "    X=data[['askPrice','askSize','bidPrice','bidSize','vwap','spread','v','return','sigma']]\n",
    "    y1=data.U\n",
    "    y2=data.D\n",
    "    \n",
    "    \n",
    "    predict_svm_up=loaded_svm_up_model.predict(X)\n",
    "    predict_lm_up=loaded_lm_up_model.predict(X)\n",
    "    predict_svm_dn=loaded_svm_dn_model.predict(X)\n",
    "    predict_lm_dn=loaded_lm_dn_model.predict(X)\n",
    "    \n",
    "    data['predict_svm_up']=predict_svm_up\n",
    "    data['predict_lm_up']=predict_lm_up\n",
    "    data['predict_svm_dn']=predict_svm_dn\n",
    "    data['predict_lm_dn']=predict_lm_dn\n",
    "    \n",
    "    data['predict_svm']=data.predict_svm_up+data.predict_svm_dn\n",
    "    data['predict_lm']=data.predict_lm_up+data.predict_lm_dn\n",
    "    \n",
    "    data['UD']=np.where(np.logical_and(data.predict_svm>0,data.predict_lm>0),1,np.where(np.logical_and(data.predict_svm<0,data.predict_lm<0),-1,0))  \n",
    "       \n",
    "    df_ml['UD']=data.UD\n",
    "\n",
    "### LSTM\n",
    "\n",
    "#df.loc[:, cols].prod(axis=1)\n",
    "def lstm_processing(df):\n",
    "    #df=df.dropna()\n",
    "    df_price=df[['mid','vwap','arima','km','REG','SVR']]\n",
    "    #normalization\n",
    "    dfn=normalise(df_price,12)\n",
    "    dfn['UD']=df.UD\n",
    "    return dfn\n",
    "\n",
    "\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas\n",
    "import math\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas\n",
    "import math\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from keras.models import load_model\n",
    "model = load_model('21sep.h5')\n",
    "\n",
    "# convert an array of values into a dataset matrix\n",
    "def create_dataset(dataset, look_back=1):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(dataset)-look_back-1):\n",
    "        a = dataset[i:(i+look_back), 0]\n",
    "        b = dataset[i:(i+look_back), 1]\n",
    "        c = dataset[i:(i+look_back), 2]\n",
    "        d = dataset[i:(i+look_back), 3]\n",
    "        e=  dataset[i:(i+look_back), 4]\n",
    "        f = dataset[i:(i+look_back), 5]\n",
    "        g=  dataset[i:(i+look_back), 6]\n",
    "        dataX.append(np.c_[b,c,d,e,f,g])\n",
    "        #dataX.append(b)\n",
    "        #dataX.append(c)\n",
    "        #dataX.append(d)\n",
    "        #dataX.append(e)\n",
    "        #dataX.concatenate((a,bT,cT,dT,eT),axis=1)\n",
    "        dataY.append(dataset[i + look_back,0])\n",
    "    return np.array(dataX), np.array(dataY)\n",
    "\n",
    "\n",
    "def strat_LSTM(df_lstm):\n",
    "    \n",
    "    #normalization\n",
    "    #df_lstm=lstm_processing(df_ml)\n",
    "    #df_lstm=df_lstm.dropna()\n",
    "    dataset=df_lstm.values\n",
    "    dataset = dataset.astype('float32')\n",
    "    # reshape into X=t and Y=t+1\n",
    "    look_back = 3\n",
    "    X_,Y_ = create_dataset(dataset,look_back)\n",
    "    \n",
    "    # reshape input to be [samples, time steps, features]\n",
    "    X_ = numpy.reshape(X_, (X_.shape[0],X_.shape[1],X_.shape[2]))\n",
    "    # make predictions\n",
    "    predict = model.predict(X_)\n",
    "    df_lstm=df_lstm.tail(len(predict))\n",
    "    df_lstm['LSTM']=predict\n",
    "\n",
    "    #LSTM=(df_lstm.LSTM*(df_ml.mid.rolling(60).max()-df_ml.midClose.rolling(60).min()))+df_LSTM.Close.rolling(60).min()\n",
    "    LSTM=de_normalise(df_ml.mid,df_lstm.LSTM,window_length=12)\n",
    "    df_ml['LSTM']=LSTM\n",
    "    \n",
    "    return LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_ml=pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final=pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/octo/anaconda2/envs/carnd-term1/lib/python3.5/site-packages/ipykernel/__main__.py:150: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "\n",
      "/home/octo/anaconda2/envs/carnd-term1/lib/python3.5/site-packages/ipykernel/__main__.py:151: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "\n",
      "/home/octo/anaconda2/envs/carnd-term1/lib/python3.5/site-packages/ipykernel/__main__.py:92: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "\n",
      "/home/octo/anaconda2/envs/carnd-term1/lib/python3.5/site-packages/ipykernel/__main__.py:93: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "\n",
      "/home/octo/anaconda2/envs/carnd-term1/lib/python3.5/site-packages/ipykernel/__main__.py:261: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                mid       vwap       arima          km  \\\n",
      "2017-09-26 20:37:08.342256  249.455  249.45985  249.455004  249.451828   \n",
      "\n",
      "                                   REG         SVR   UD        LSTM  \n",
      "2017-09-26 20:37:08.342256  249.455007  249.454317  0.0  249.452379  \n",
      "                                mid       vwap       arima          km  \\\n",
      "2017-09-26 20:37:08.342256  249.455  249.45985  249.455004  249.451828   \n",
      "\n",
      "                                   REG         SVR   UD        LSTM  \n",
      "2017-09-26 20:37:08.342256  249.455007  249.454317  0.0  249.452379  \n",
      "                                mid       vwap       arima          km  \\\n",
      "2017-09-26 20:37:08.342256  249.455  249.45985  249.455004  249.451828   \n",
      "\n",
      "                                   REG         SVR   UD  LSTM  \n",
      "2017-09-26 20:37:08.342256  249.455007  249.454317  0.0   NaN  \n",
      "                                mid       vwap       arima          km  \\\n",
      "2017-09-26 20:37:08.342256  249.455  249.45985  249.455004  249.451828   \n",
      "\n",
      "                                   REG         SVR   UD  LSTM  \n",
      "2017-09-26 20:37:08.342256  249.455007  249.454317  0.0   NaN  \n",
      "                                mid       vwap       arima          km  \\\n",
      "2017-09-26 20:37:08.342256  249.455  249.45985  249.455004  249.451828   \n",
      "\n",
      "                                   REG         SVR   UD  LSTM  \n",
      "2017-09-26 20:37:08.342256  249.455007  249.454317  0.0   NaN  \n",
      "                                mid       vwap       arima          km  \\\n",
      "2017-09-26 20:37:08.342256  249.455  249.45985  249.455004  249.451828   \n",
      "\n",
      "                                   REG         SVR   UD  LSTM  \n",
      "2017-09-26 20:37:08.342256  249.455007  249.454317  0.0   NaN  \n",
      "                                mid       vwap       arima          km  \\\n",
      "2017-09-26 20:37:08.342256  249.455  249.45985  249.455004  249.451828   \n",
      "\n",
      "                                   REG         SVR   UD  LSTM  \n",
      "2017-09-26 20:37:08.342256  249.455007  249.454317  0.0   NaN  \n",
      "                                mid       vwap       arima          km  \\\n",
      "2017-09-26 20:37:08.342256  249.455  249.45985  249.455004  249.451828   \n",
      "\n",
      "                                   REG         SVR   UD  LSTM  \n",
      "2017-09-26 20:37:08.342256  249.455007  249.454317  0.0   NaN  \n",
      "                                mid       vwap       arima          km  \\\n",
      "2017-09-26 20:37:08.342256  249.455  249.45985  249.455004  249.451828   \n",
      "\n",
      "                                   REG         SVR   UD  LSTM  \n",
      "2017-09-26 20:37:08.342256  249.455007  249.454317  0.0   NaN  \n",
      "                                mid       vwap       arima          km  \\\n",
      "2017-09-26 20:37:08.342256  249.455  249.45985  249.455004  249.451828   \n",
      "\n",
      "                                   REG         SVR   UD  LSTM  \n",
      "2017-09-26 20:37:08.342256  249.455007  249.454317  0.0   NaN  \n",
      "                                mid       vwap       arima          km  \\\n",
      "2017-09-26 20:37:08.342256  249.455  249.45985  249.455004  249.451828   \n",
      "\n",
      "                                   REG         SVR   UD  LSTM  \n",
      "2017-09-26 20:37:08.342256  249.455007  249.454317  0.0   NaN  \n",
      "                                mid       vwap       arima          km  \\\n",
      "2017-09-26 20:37:08.342256  249.455  249.45985  249.455004  249.451828   \n",
      "\n",
      "                                   REG         SVR   UD  LSTM  \n",
      "2017-09-26 20:37:08.342256  249.455007  249.454317  0.0   NaN  \n",
      "                                mid       vwap       arima          km  \\\n",
      "2017-09-26 20:37:08.342256  249.455  249.45985  249.455004  249.451827   \n",
      "\n",
      "                                   REG         SVR   UD  LSTM  \n",
      "2017-09-26 20:37:08.342256  249.455007  249.454317  0.0   NaN  \n",
      "                                mid       vwap       arima          km  \\\n",
      "2017-09-26 20:37:08.342256  249.455  249.45985  249.455004  249.451827   \n",
      "\n",
      "                                   REG         SVR   UD  LSTM  \n",
      "2017-09-26 20:37:08.342256  249.455007  249.454317  0.0   NaN  \n",
      "                                mid       vwap       arima          km  \\\n",
      "2017-09-26 20:37:08.342256  249.455  249.45985  249.455004  249.451827   \n",
      "\n",
      "                                   REG         SVR   UD  LSTM  \n",
      "2017-09-26 20:37:08.342256  249.455007  249.454317  0.0   NaN  \n",
      "                                mid       vwap       arima          km  \\\n",
      "2017-09-26 20:37:08.342256  249.455  249.45985  249.455004  249.451827   \n",
      "\n",
      "                                   REG         SVR   UD  LSTM  \n",
      "2017-09-26 20:37:08.342256  249.455007  249.454317  0.0   NaN  \n",
      "                                mid       vwap       arima          km  \\\n",
      "2017-09-26 20:37:08.342256  249.455  249.45985  249.455004  249.451827   \n",
      "\n",
      "                                   REG         SVR   UD  LSTM  \n",
      "2017-09-26 20:37:08.342256  249.455007  249.454317  0.0   NaN  \n",
      "                                mid       vwap  arima          km         REG  \\\n",
      "2017-09-26 20:37:08.342256  249.455  249.45985    NaN  249.451827  249.455007   \n",
      "\n",
      "                                   SVR   UD  LSTM  \n",
      "2017-09-26 20:37:08.342256  249.454317  0.0   NaN  \n",
      "                                mid       vwap  arima          km         REG  \\\n",
      "2017-09-26 20:37:08.342256  249.455  249.45985    NaN  249.451827  249.455007   \n",
      "\n",
      "                                   SVR   UD  LSTM  \n",
      "2017-09-26 20:37:08.342256  249.454317  0.0   NaN  \n",
      "                                mid       vwap  arima          km  REG  SVR  \\\n",
      "2017-09-26 20:37:08.342256  249.455  249.45985    NaN  249.451826  NaN  NaN   \n",
      "\n",
      "                             UD  LSTM  \n",
      "2017-09-26 20:37:08.342256  0.0   NaN  \n"
     ]
    }
   ],
   "source": [
    "## warm up upto preprocessing\n",
    "#final=pd.DataFrame()\n",
    "window=20\n",
    "for _ in range(window):\n",
    "#while True:\n",
    "    iterations += 1\n",
    "    string = socket_sub.recv_string()\n",
    "    sym, bidPrice,bidSize,askPrice,askSize = string.split()\n",
    "    #print('%s %s %s %s %s' % (sym, bidPrice,bidSize,askPrice,askSize))\n",
    "    dt = datetime.datetime.now()\n",
    "    df = df.append(pd.DataFrame({'Stock':sym,'bidPrice': float(bidPrice),'bidSize': float(bidSize),'askPrice': float(askPrice),'askSize': float(askSize)},index=[dt]))\n",
    "    df=preprocessing(df)\n",
    "    df=df.tail(200)\n",
    "    #df=df.tail(100)\n",
    "    \n",
    "    data=df[['askPrice','askSize','bidPrice','bidSize','mid','vwap','spread','v','return','sigma']]\n",
    "    \n",
    "    df_ml['mid']=df.mid\n",
    "    df_ml['vwap']=df.vwap\n",
    "    \n",
    "    df_arima=arima_processing(df)\n",
    "    df_ml['arima']=ARIMA_(df_arima)  \n",
    "    \n",
    "    dfn=normalise(data)\n",
    "    #dfn=normalise_z(data)\n",
    "    \n",
    "    kalman_ma(data)\n",
    "    strat_lr(data,dfn)\n",
    "    \n",
    "    df_cl=data_class(data)\n",
    "    classification_up_dn(df_cl)\n",
    "    \n",
    "    df_lstm=lstm_processing(df_ml)\n",
    "    df_lstm=strat_LSTM(df_lstm.tail(60))\n",
    "    final['lstm']=df_lstm\n",
    "    final['mid']=df.mid\n",
    "    #final['stock']=df.Stock\n",
    "    #final.insert(loc=0, column='Stock', value=df.Stock)\n",
    "    \n",
    "    #df_ml.insert(loc=0, column='Stock', value=df.Stock)\n",
    "    \n",
    "    #print(df.tail(1))\n",
    "    #print(data.tail(1))\n",
    "    #print(df_arima.tail(1))\n",
    "    #print(df_ARIMA.tail(1))\n",
    "    #print(dfn.tail(1))\n",
    "    #print(df_cl.tail(1))\n",
    "    #print(df_lstm[-1])\n",
    "    print(df_ml.tail(1))\n",
    "    #print(final.tail(1))\n",
    " \n",
    "    \n",
    "    \n",
    "    #x = df_ml.to_string(header=False,index=False,index_names=False).split('\\n')\n",
    "    #socket_pub.send_string(x[-1])\n",
    "    #print(x[-1]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_ml.dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2017-09-26 20:16:21.133837           NaN\n",
       "2017-09-26 20:16:22.222928           NaN\n",
       "2017-09-26 20:16:23.366308           NaN\n",
       "2017-09-26 20:16:24.638819           NaN\n",
       "2017-09-26 20:16:25.902099           NaN\n",
       "2017-09-26 20:16:27.129665           NaN\n",
       "2017-09-26 20:16:28.286048           NaN\n",
       "2017-09-26 20:16:29.365283           NaN\n",
       "2017-09-26 20:16:30.431577           NaN\n",
       "2017-09-26 20:16:31.566511           NaN\n",
       "2017-09-26 20:16:32.808600           NaN\n",
       "2017-09-26 20:16:34.051606           NaN\n",
       "2017-09-26 20:16:35.358547           NaN\n",
       "2017-09-26 20:16:36.535595           NaN\n",
       "2017-09-26 20:16:37.679515           NaN\n",
       "2017-09-26 20:16:38.756362           NaN\n",
       "2017-09-26 20:16:39.869944           NaN\n",
       "2017-09-26 20:16:41.077430           NaN\n",
       "2017-09-26 20:16:42.309531           NaN\n",
       "2017-09-26 20:16:43.602571           NaN\n",
       "2017-09-26 20:16:44.737715           NaN\n",
       "2017-09-26 20:16:45.814190           NaN\n",
       "2017-09-26 20:16:46.889881           NaN\n",
       "2017-09-26 20:16:48.075408           NaN\n",
       "2017-09-26 20:16:49.372491           NaN\n",
       "2017-09-26 20:16:50.617259           NaN\n",
       "2017-09-26 20:16:51.822013           NaN\n",
       "2017-09-26 20:16:52.897775           NaN\n",
       "2017-09-26 20:16:53.978959           NaN\n",
       "2017-09-26 20:16:55.103712           NaN\n",
       "                                 ...    \n",
       "2017-09-26 20:20:09.233753           NaN\n",
       "2017-09-26 20:20:10.533481           NaN\n",
       "2017-09-26 20:20:11.842414           NaN\n",
       "2017-09-26 20:20:13.370989           NaN\n",
       "2017-09-26 20:20:14.841341    249.484915\n",
       "2017-09-26 20:20:16.247970    249.484957\n",
       "2017-09-26 20:20:17.568219    249.484957\n",
       "2017-09-26 20:20:18.923466    249.484926\n",
       "2017-09-26 20:20:20.267639    249.473709\n",
       "2017-09-26 20:20:21.799113    249.474347\n",
       "2017-09-26 20:20:23.286076    249.475270\n",
       "2017-09-26 20:20:24.728134    249.475177\n",
       "2017-09-26 20:20:26.106949    249.475072\n",
       "2017-09-26 20:20:27.453954    249.475040\n",
       "2017-09-26 20:20:28.872863    249.475156\n",
       "2017-09-26 20:20:30.357153    249.475093\n",
       "2017-09-26 20:20:31.807530    249.475076\n",
       "2017-09-26 20:20:33.137101    249.475011\n",
       "2017-09-26 20:20:34.419325    249.480350\n",
       "2017-09-26 20:20:35.769088    249.485784\n",
       "2017-09-26 20:20:37.267215    249.485461\n",
       "2017-09-26 20:20:38.756267    249.485104\n",
       "2017-09-26 20:20:40.224671    249.485078\n",
       "2017-09-26 20:20:41.590396    249.485032\n",
       "2017-09-26 20:20:42.935493    249.479688\n",
       "2017-09-26 20:20:44.303235    249.475029\n",
       "2017-09-26 20:20:45.790077    249.475126\n",
       "2017-09-26 20:20:47.485723    249.475135\n",
       "2017-09-26 20:20:56.121532    249.475187\n",
       "2017-09-26 20:24:57.711879    249.475137\n",
       "Length: 200, dtype: float64"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
